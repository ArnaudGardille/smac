Failure # 1 (occurred at 2023-05-03_14-39-09)
Traceback (most recent call last):
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/tune/trial_runner.py", line 890, in _process_trial
    results = self.trial_executor.fetch_result(trial)
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py", line 788, in fetch_result
    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/worker.py", line 1627, in get
    raise value
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, [36mray::MAPPOTrainer.__init__()[39m (pid=40661, ip=172.20.10.4)
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 137, in __init__
    Trainer.__init__(self, config, env, logger_creator)
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 623, in __init__
    super().__init__(config, logger_creator)
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/tune/trainable.py", line 107, in __init__
    self.setup(copy.deepcopy(self.config))
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 147, in setup
    super().setup(config)
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 776, in setup
    self._init(self.config, self.env_creator)
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 171, in _init
    self.workers = self._make_workers(
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 858, in _make_workers
    return WorkerSet(
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py", line 110, in __init__
    self._local_worker = self._make_worker(
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py", line 406, in _make_worker
    worker = cls(
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py", line 584, in __init__
    self._build_policy_map(
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py", line 1384, in _build_policy_map
    self.policy_map.create_policy(name, orig_cls, obs_space, act_space,
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/rllib/policy/policy_map.py", line 143, in create_policy
    self[policy_id] = class_(observation_space, action_space,
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/rllib/policy/policy_template.py", line 257, in __init__
    self.parent_cls.__init__(
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py", line 214, in __init__
    self.model_gpu_towers.append(model_copy.to(self.devices[i]))
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/torch/nn/modules/module.py", line 852, in to
    return self._apply(convert)
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/torch/nn/modules/module.py", line 530, in _apply
    module._apply(fn)
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 189, in _apply
    self.flatten_parameters()
  File "/home/nono/.conda/envs/marllib/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 175, in flatten_parameters
    torch._cudnn_rnn_flatten_weight(
RuntimeError: CUDA error: no kernel image is available for execution on the device
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

